import torch as t
from torch.utils.tensorboard import SummaryWriter
from tqdm.autonotebook import tqdm
import os
from sklearn.metrics import f1_score  # å¼•å…¥ f1_score è®¡ç®—æŒ‡æ ‡

class Trainer:
    def __init__(self,
                 model,                        # è®­ç»ƒçš„æ¨¡å‹
                 crit,                         # æŸå¤±å‡½æ•°
                 optim=None,                   # ä¼˜åŒ–å™¨
                 train_dl=None,                # è®­ç»ƒæ•°æ®é›†
                 val_test_dl=None,             # éªŒè¯/æµ‹è¯•æ•°æ®é›†
                 cuda=True,                    # æ˜¯å¦ä½¿ç”¨ GPU
                 early_stopping_patience=10,   # æ—©åœç­–ç•¥
                 log_dir="runs/experiment"):   # TensorBoard æ—¥å¿—ç›®å½•
        
        self._model = model
        self._crit = crit
        self._optim = optim
        self._train_dl = train_dl
        self._val_test_dl = val_test_dl
        self._cuda = cuda
        self._early_stopping_patience = early_stopping_patience

        if cuda:
            self._model = model.cuda()
            self._crit = crit.cuda()

        # ğŸ”¥ ç¡®ä¿ TensorBoard æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(log_dir, exist_ok=True)
        self.writer = SummaryWriter(log_dir)

    def save_checkpoint(self, epoch):
        os.makedirs("checkpoints", exist_ok=True)  # ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å­˜åœ¨
        t.save({'state_dict': self._model.state_dict()}, 
               f'checkpoints/checkpoint_{epoch:03d}.ckp')

    def restore_checkpoint(self, epoch_n):
        ckp = t.load(f'checkpoints/checkpoint_{epoch_n:03d}.ckp',
                     map_location='cuda' if self._cuda else 'cpu')
        self._model.load_state_dict(ckp['state_dict'])

    def save_onnx(self, fn):
        m = self._model.cpu()
        m.eval()
        x = t.randn(1, 3, 300, 300, requires_grad=True)
        y = self._model(x)
        t.onnx.export(m,  # model being run
                      x,  # model input (or a tuple for multiple inputs)
                      fn,  # where to save the model (can be a file or file-like object)
                      export_params=True,  # store the trained parameter weights inside the model file
                      opset_version=10,  # the ONNX version to export the model to
                      do_constant_folding=True,  # whether to execute constant folding for optimization
                      input_names=['input'],  # the model's input names
                      output_names=['output'],  # the model's output names
                      dynamic_axes={'input': {0: 'batch_size'},  # variable lenght axes
                                    'output': {0: 'batch_size'}})

    def train_step(self, x, y):
        self._model.train()
        self._optim.zero_grad()
        pred = self._model(x)
        loss = self._crit(pred, y)
        loss.backward()
        
        # æ·»åŠ æ¢¯åº¦è£å‰ªï¼ˆæ§åˆ¶æ¢¯åº¦æœ€å¤§èŒƒæ•°ä¸º1.0ï¼‰
        t.nn.utils.clip_grad_norm_(self._model.parameters(), max_norm=2.0)
        
        self._optim.step()
        return loss.item()

    def val_test_step(self, x, y):
        self._model.eval()
        with t.no_grad():
            pred = self._model(x)
            loss = self._crit(pred, y)
        return loss.item(), pred

    def train_epoch(self, epoch):
        self._model.train()
        total_loss = 0.0
        count = 0
        for x, y in tqdm(self._train_dl, desc=f"Training Epoch {epoch+1}"):
            if self._cuda:
                x, y = x.cuda(), y.cuda()
            loss = self.train_step(x, y)
            total_loss += loss
            count += 1
        avg_loss = total_loss / count if count > 0 else 0.0
        self.writer.add_scalar("Loss/Train", avg_loss, epoch)  # âœ… è®°å½•è®­ç»ƒæŸå¤±
        return avg_loss

    def val_test(self, epoch):
        self._model.eval()
        total_loss = 0.0
        count = 0
        
        # ç”¨äºå­˜å‚¨æ‰€æœ‰é¢„æµ‹å’Œæ ‡ç­¾
        all_preds = []
        all_labels = []
        
        with t.no_grad():
            for x, y in tqdm(self._val_test_dl, desc=f"Validating Epoch {epoch+1}"):
                if self._cuda:
                    x, y = x.cuda(), y.cuda()
                loss, pred = self.val_test_step(x, y)
                total_loss += loss
                count += 1
                
                # å°†å½“å‰ batch çš„é¢„æµ‹å’Œæ ‡ç­¾ä¿å­˜èµ·æ¥ï¼Œé¢„æµ‹ç»“æœå››èˆäº”å…¥å¾—åˆ° 0 æˆ– 1
                # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾æ¨¡å‹è¾“å‡ºå½¢çŠ¶é€‚åˆç›´æ¥ round() å¾—åˆ°ç±»åˆ«ï¼ˆå¦‚äºŒåˆ†ç±»é—®é¢˜ï¼‰
                all_preds.append(pred.cpu().round())
                all_labels.append(y.cpu())
                
        avg_loss = total_loss / count if count > 0 else 0.0
        
        # æ‹¼æ¥æ‰€æœ‰ batch çš„é¢„æµ‹å’Œæ ‡ç­¾
        all_preds = t.cat(all_preds, dim=0).squeeze()
        all_labels = t.cat(all_labels, dim=0).squeeze()
        
        # å°† tensor è½¬ä¸º numpy æ•°ç»„å¹¶è®¡ç®— F1 Score
        f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average='weighted')
        self.writer.add_scalar("F1/Validation", f1, epoch)  # âœ… è®°å½•éªŒè¯é›† F1 åˆ†æ•°
        self.writer.add_scalar("Loss/Validation", avg_loss, epoch)  # âœ… è®°å½•éªŒè¯æŸå¤±
        
        print(f"Validation Epoch {epoch+1}: Avg Loss: {avg_loss:.4f}, F1 Score: {f1:.4f}")
        return avg_loss, f1

    def fit(self, epochs=10, scheduler=None):
        best_val_loss = float('inf')
        patience_counter = 0
        train_losses, val_losses, f1_scores = [], [], []
        for epoch in range(epochs):
            # è®­ç»ƒä¸€ä¸ª epoch
            train_loss = self.train_epoch(epoch)
            
            # éªŒè¯ä¸€ä¸ª epoch
            val_loss, f1 = self.val_test(epoch)
            
            # è®°å½•æŸå¤±å’Œ F1 åˆ†æ•°
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            f1_scores.append(f1)
            
            # è°ƒç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚æœæä¾›äº† schedulerï¼‰
            if scheduler is not None:
                scheduler.step(val_loss)  # æ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
            
            # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch)
            
            # æ‰“å°å½“å‰ epoch çš„ç»“æœ
            print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - F1 Score: {f1:.4f}")
            if val_loss < best_val_loss*0.995:
                best_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter +=1
                if patience_counter >= 20:
                    print(f"Early stopping at epoch {epoch+1}")
                    break
        # è®­ç»ƒç»“æŸåå…³é—­ TensorBoard
        self.writer.close()
        
        # è¿”å›è®­ç»ƒå’ŒéªŒè¯çš„æŸå¤±ä»¥åŠ F1 åˆ†æ•°
        return train_losses, val_losses, f1_scores